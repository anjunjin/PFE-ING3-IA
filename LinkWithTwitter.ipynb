{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinkWithTwitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjunjin/PFE-ING3-IA/blob/master/LinkWithTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ndr1ImFEo6w"
      },
      "source": [
        "Installation de la library *python-dotenv*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ1BtietFVX_",
        "outputId": "273d8e8e-2d4c-48ed-bd05-46e143d6f08c"
      },
      "source": [
        "!pip install python-dotenv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (0.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYWW5ymDHlMF"
      },
      "source": [
        "Importation des libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvM0ew0bDEzj"
      },
      "source": [
        "import tweepy\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import dotenv_values\n",
        "import requests\n",
        "from IPython.display import JSON"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnZdcHeFysW"
      },
      "source": [
        "Liaison au Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJZaYlBdx7p",
        "outputId": "ba58a03f-f4b8-4cc2-d4e3-fea9ef93b7aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive/PFE/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISVEBKTDF2jm"
      },
      "source": [
        "R√©cup√©ration dans ***data*** des donn√©s n√©cessaires de tableauSites.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV8keOGed_Zd"
      },
      "source": [
        "data = pd.read_csv(DATA_PATH+\"tableauSites.csv\", encoding='utf-8').drop(['Unnamed: 0', 'id'], axis=1).values.tolist()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxAomGqCGFDt"
      },
      "source": [
        "R√©cup√©ration des cl√©s pour se connecter au compte Twitter Developers et utiliser son API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM4AMrZ1DLZT"
      },
      "source": [
        "API_TOKEN = pd.read_csv(DATA_PATH+\"Cl√©s.csv\", encoding='utf-8', sep=',')['Bearer Token'][0]\n",
        "API_KEY = pd.read_csv(DATA_PATH+\"Cl√©s.csv\", encoding='utf-8', sep=',')['API Key'][0]\n",
        "API_SECRET = pd.read_csv(DATA_PATH+\"Cl√©s.csv\", encoding='utf-8', sep=',')['API Key Secret'][0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjZKlj2GSzN"
      },
      "source": [
        "Authentifiation au compte Twitter Developers et r√©cup√©ration de l'API tweepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4cck9VECa5"
      },
      "source": [
        "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoDfaEzGvqE"
      },
      "source": [
        "Cr√©ation d'une classe TwitterUser contenant les fonctions de r√©cup√©ration de donn√©es (utilisateurs, tweets, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt1VhX0_ENen"
      },
      "source": [
        "    class TwitterUser():\n",
        "        _screen_name = \"\"\n",
        "        _id = \"\"\n",
        "        _user_data = None\n",
        "        followers = []\n",
        "        following = []\n",
        "        tweet = []\n",
        "        like = []\n",
        "        request_count = 0\n",
        "\n",
        "        def __init__(self, sname = \"\", _json = {}):\n",
        "            if (sname != \"\"):\n",
        "                self._screen_name = sname\n",
        "                self.get_info()\n",
        "            if (_json != {}):\n",
        "                self._json = _json\n",
        "\n",
        "        def get_info(self):\n",
        "            try:\n",
        "                self._user_data = api.get_user(screen_name=self._screen_name)\n",
        "            except Exception as e:\n",
        "                print(\"Exception get_info\")\n",
        "\n",
        "        def get_all_data(self):\n",
        "            self.get_tweets()\n",
        "            # self.get_followers()\n",
        "            # self.get_following()\n",
        "            # self.get_like()\n",
        "\n",
        "        def set_name(self, name):\n",
        "            self._screen_name = name\n",
        "\n",
        "        def set_id(self, _id):\n",
        "            self._id = _id\n",
        "\n",
        "        def get_followers(self):\n",
        "            with tqdm(total=self._user_data.followers_count, desc=\"get_followers for \" + self._user_data.screen_name) as pbar:\n",
        "                for page in tweepy.Cursor(api.followers, screen_name=self._screen_name, count=200).pages():\n",
        "                    self.followers.extend(page)\n",
        "                    pbar.update(len(page))\n",
        "\n",
        "\n",
        "        def get_following(self):\n",
        "            with tqdm(total=self._user_data.friends_count, desc=\"get_following for \" + self._user_data.screen_name) as pbar:\n",
        "                for page in tweepy.Cursor(api.friends, screen_name=self._screen_name, count=200).pages():\n",
        "                    self.following.extend(page)\n",
        "                    pbar.update(len(page))\n",
        "\n",
        "        # on ne peut prendre que les 3000 derniers\n",
        "        def get_tweets(self):\n",
        "            try:\n",
        "                with tqdm(total=1000, desc=\"get_tweets for \" + self._user_data.screen_name) as pbar:\n",
        "                    for page in tweepy.Cursor(api.user_timeline, screen_name=self._screen_name, count=200).pages():\n",
        "                        self.tweet.extend(page)\n",
        "                        pbar.update(len(page))\n",
        "            except Exception as e:\n",
        "                print(\"Exception get_tweets\")\n",
        "\n",
        "        # on ne peut prendre que les 3000 derniers\n",
        "        def get_like(self):\n",
        "            if self._user_data.protected:\n",
        "                print(\"can get like from \" + self._user_data.screen_name + \", the account is protected\")\n",
        "            else:\n",
        "                with tqdm(total=3000, desc=\"get_likes for \" + self._user_data.screen_name) as pbar:\n",
        "                    for page in tweepy.Cursor(api.favorites, screen_name=self._screen_name, count=200).pages():\n",
        "                        self.like.extend(page)\n",
        "                        pbar.update(len(page))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-U_RpJEp5by"
      },
      "source": [
        "Division de la liste des site en plusieurs sous liste pour ne pas ex√©der la RAM allou√© par Google Colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHt_5sDp32s"
      },
      "source": [
        "def split_list(a_list):\n",
        "    half = len(a_list)//2\n",
        "    return a_list[:half], a_list[half:]\n",
        "\n",
        "dataA, dataB = split_list(data)\n",
        "data1, data2 = split_list(dataA)\n",
        "data3, data4 = split_list(dataB)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfLfTerE4v8w"
      },
      "source": [
        "Ajout de tous les tweets dans un tableau qui va contenir les donn√©es importantes de chaque tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ma3JwJwD4vn6",
        "outputId": "ebd71b25-b379-4db3-c11b-18d7a2fd51f4"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data1:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "tableauTweets = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "tableauTweets"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for france_soir: 3213it [00:07, 408.84it/s]\n",
            "get_tweets for ThierryRegenere:  38%|‚ñà‚ñà‚ñà‚ñä      | 375/1000 [00:00<00:01, 464.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for EmaKrusi:  32%|‚ñà‚ñà‚ñà‚ñè      | 315/1000 [00:01<00:02, 303.49it/s]\n",
            "get_tweets for LLP_Le_Vrai: 3040it [00:10, 276.84it/s]\n",
            "get_tweets for covid_infos:  27%|‚ñà‚ñà‚ñã       | 271/1000 [00:00<00:02, 312.69it/s]\n",
            "get_tweets for cab2626: 1911it [00:04, 392.57it/s]\n",
            "get_tweets for lamentableFR: 2123it [00:04, 508.14it/s]\n",
            "get_tweets for Elishean: 3250it [00:06, 477.09it/s]\n",
            "get_tweets for DrMo7oG: 2984it [00:08, 372.53it/s]\n",
            "get_tweets for yetiblog: 3114it [00:09, 341.80it/s]\n",
            "get_tweets for lepolitique_fr:   0%|          | 1/1000 [00:00<05:35,  2.98it/s]\n",
            "get_tweets for Les_Repliques: 3250it [00:07, 410.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for Jnewsfoot: 3192it [00:07, 442.52it/s]\n",
            "get_tweets for JournalElysee: 1107it [00:02, 410.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for Golfinvestigat1:   4%|‚ñç         | 41/1000 [00:00<00:09, 105.63it/s]\n",
            "get_tweets for iiiprs: 1540it [00:04, 348.84it/s]\n",
            "get_tweets for voxdotcom: 3248it [00:08, 392.06it/s]\n",
            "get_tweets for usinenouvelle: 3250it [00:06, 475.32it/s]\n",
            "get_tweets for Observateurs: 3250it [00:07, 423.87it/s]\n",
            "get_tweets for RadioTeleSuisse: 3099it [00:08, 378.34it/s]\n",
            "get_tweets for RTBF: 3250it [00:06, 486.12it/s]\n",
            "get_tweets for lalibrebe: 3250it [00:08, 393.34it/s]\n",
            "get_tweets for brutofficiel: 3249it [00:07, 433.55it/s]\n",
            "get_tweets for EpochTimesFR: 3249it [00:07, 411.03it/s]\n",
            "get_tweets for FR_Conversation: 3250it [00:06, 464.52it/s]\n",
            "get_tweets for Actu17: 3250it [00:06, 470.81it/s]\n",
            "get_tweets for SciencePost_fr: 3200it [00:06, 510.67it/s]\n",
            "get_tweets for RevPermanente: 3250it [00:07, 430.23it/s]\n",
            "get_tweets for APRNEWS1: 3234it [00:07, 443.87it/s]\n",
            "get_tweets for AfriPulse_FR:   8%|‚ñä         | 76/1000 [00:00<00:06, 146.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n",
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for LBDTSS: 3200it [00:06, 509.49it/s]\n",
            "get_tweets for le_journalnews:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/1000 [00:01<00:00, 472.99it/s]\n",
            "get_tweets for FraicheGazette: 1325it [00:03, 430.35it/s]\n",
            "get_tweets for onaimelafrance:  37%|‚ñà‚ñà‚ñà‚ñã      | 373/1000 [00:01<00:01, 323.06it/s]\n",
            "get_tweets for Daily_Express: 3250it [00:07, 461.12it/s]\n",
            "get_tweets for LeDesavantage:  38%|‚ñà‚ñà‚ñà‚ñä      | 377/1000 [00:00<00:01, 389.23it/s]\n",
            "get_tweets for Liberaphion:   3%|‚ñé         | 26/1000 [00:00<00:13, 73.63it/s] \n",
            "get_tweets for ThomasJoly60: 3135it [00:09, 322.73it/s]\n",
            "get_tweets for EuroScoop_FR: 3200it [00:06, 496.54it/s]\n",
            "get_tweets for DrWakefield: 2073it [00:05, 366.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for Limportant_fr: 3250it [00:07, 453.47it/s]\n",
            "get_tweets for worldtvdesinfo: 2195it [00:06, 346.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for leouestfranc: 3242it [00:09, 341.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n",
            "Exception get_info\n",
            "Exception get_tweets\n",
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for David_vanH: 3224it [00:09, 335.02it/s]\n",
            "get_tweets for PrisonPlanet: 3234it [00:09, 355.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n",
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for delitdimages: 3200it [00:09, 354.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for OignonCantal:  11%|‚ñà         | 108/1000 [00:00<00:04, 210.95it/s]\n",
            "get_tweets for wucnews: 3198it [00:06, 513.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for tomimagfr: 1474it [00:03, 446.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for BuzzArenaFr: 1589it [00:03, 415.94it/s]\n",
            "get_tweets for ActumagInfo: 3200it [00:06, 465.76it/s]\n",
            "get_tweets for francetvdesinfo:   0%|          | 1/1000 [00:00<04:16,  3.90it/s]\n",
            "get_tweets for AstuceDuJourMag:   0%|          | 3/1000 [00:00<01:28, 11.21it/s]\n",
            "get_tweets for actualidadpanam: 3228it [00:08, 384.26it/s]\n",
            "get_tweets for DailySnark: 2484it [00:05, 474.49it/s]\n",
            "get_tweets for FreeWoodPost: 1258it [00:02, 432.48it/s]\n",
            "get_tweets for The_Postillon:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/1000 [00:01<00:01, 351.55it/s]\n",
            "get_tweets for thedailymash: 3250it [00:07, 446.11it/s]\n",
            "get_tweets for akami_shimbun:   5%|‚ñç         | 46/1000 [00:00<00:07, 123.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for ravelationsmag: 2506it [00:05, 465.02it/s]\n",
            "get_tweets for InvestigAction: 3231it [00:08, 370.96it/s]\n",
            "get_tweets for AlertesNews:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "get_tweets for cyceon_fr:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "get_tweets for Gamekult: 3249it [00:07, 422.56it/s]\n",
            "get_tweets for Causeur: 3244it [00:07, 426.56it/s]\n",
            "get_tweets for SecretNewsInfo: 3109it [00:07, 421.68it/s]\n",
            "get_tweets for Envie_devivre:   4%|‚ñç         | 38/1000 [00:00<00:10, 94.88it/s] \n",
            "get_tweets for RidiculeTV:  38%|‚ñà‚ñà‚ñà‚ñä      | 376/1000 [00:01<00:01, 347.96it/s]\n",
            "get_tweets for Filteris: 3216it [00:09, 341.74it/s]\n",
            "get_tweets for Europe_Israel: 3135it [00:08, 354.48it/s]\n",
            "get_tweets for Civilwarineurop: 2426it [00:09, 261.54it/s]\n",
            "get_tweets for spiritsciences: 3205it [00:07, 438.97it/s]\n",
            "get_tweets for RadioCockpit: 1938it [00:04, 454.66it/s]\n",
            "get_tweets for corsemachin:   5%|‚ñå         | 50/1000 [00:00<00:08, 108.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for FoxNews: 3250it [00:06, 466.96it/s]\n",
            "get_tweets for i24NEWS_FR: 3250it [00:12, 259.07it/s]\n",
            "get_tweets for Lesjoursfr: 3250it [00:09, 339.73it/s]\n",
            "get_tweets for nextinpact: 3250it [00:07, 409.35it/s]\n",
            "get_tweets for Rue89Bordeaux: 3249it [00:07, 446.51it/s]\n",
            "get_tweets for lesoir: 3250it [00:07, 438.96it/s]\n",
            "get_tweets for conspiration: 3248it [00:09, 359.63it/s]\n",
            "get_tweets for Korben: 2017it [00:04, 441.56it/s]\n",
            "get_tweets for leLab_E1: 3200it [00:06, 468.94it/s]\n",
            "get_tweets for Qofficiel: 3250it [00:07, 432.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for lundimat1: 3228it [00:08, 378.03it/s]\n",
            "get_tweets for Fakir_: 3248it [00:09, 339.05it/s]\n",
            "get_tweets for PaulJorion: 3249it [00:07, 449.77it/s]\n",
            "get_tweets for cahiersdufoot: 3240it [00:08, 377.68it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>id_tweet</th>\n",
              "      <th>tweet</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>label_liability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78957336</td>\n",
              "      <td>1464256415372034074</td>\n",
              "      <td>üëÄ Des professeurs ni√ßois ont d√©couvert des cam...</td>\n",
              "      <td>38</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>78957336</td>\n",
              "      <td>1464229853205602312</td>\n",
              "      <td>RT @CStrateges: Revue de presse :\\nL'√©pid√©miol...</td>\n",
              "      <td>235</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78957336</td>\n",
              "      <td>1464228383085240320</td>\n",
              "      <td>üèéÔ∏è Objectif z√©ro mort sur les routes en 2050 :...</td>\n",
              "      <td>60</td>\n",
              "      <td>89</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78957336</td>\n",
              "      <td>1464221914847657988</td>\n",
              "      <td>Et notre entretien, √† voir ou revoir üëá\\nhttps:...</td>\n",
              "      <td>29</td>\n",
              "      <td>73</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78957336</td>\n",
              "      <td>1464221524102160427</td>\n",
              "      <td>üì£ \"Nous voici donc entr√©s dans une nouvelle zo...</td>\n",
              "      <td>68</td>\n",
              "      <td>133</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10378692</th>\n",
              "      <td>96419861</td>\n",
              "      <td>1217018179823730689</td>\n",
              "      <td>N√©anmoins, dans le Top 20, quelques clubs font...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10378693</th>\n",
              "      <td>96419861</td>\n",
              "      <td>1217017969366249473</td>\n",
              "      <td>Selon le rapport annuel Deloitte, les revenus ...</td>\n",
              "      <td>16</td>\n",
              "      <td>38</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10378694</th>\n",
              "      <td>96419861</td>\n",
              "      <td>1216978446565494784</td>\n",
              "      <td>RT @FlorentToniutti: Coucou, c'est bien @floto...</td>\n",
              "      <td>468</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10378695</th>\n",
              "      <td>96419861</td>\n",
              "      <td>1216814345390383104</td>\n",
              "      <td>@yvanmaugere Nous n'avons re√ßu qu'un message :...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10378696</th>\n",
              "      <td>96419861</td>\n",
              "      <td>1216765351297474560</td>\n",
              "      <td>RT @foot_je: Vous avez aim√© le graphe Ligue 1,...</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10378697 rows √ó 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           user_id             id_tweet  ... favorite_count  label_liability\n",
              "0         78957336  1464256415372034074  ...             72                2\n",
              "1         78957336  1464229853205602312  ...              0                2\n",
              "2         78957336  1464228383085240320  ...             89                2\n",
              "3         78957336  1464221914847657988  ...             73                2\n",
              "4         78957336  1464221524102160427  ...            133                2\n",
              "...            ...                  ...  ...            ...              ...\n",
              "10378692  96419861  1217018179823730689  ...              5                4\n",
              "10378693  96419861  1217017969366249473  ...             38                4\n",
              "10378694  96419861  1216978446565494784  ...              0                4\n",
              "10378695  96419861  1216814345390383104  ...              0                4\n",
              "10378696  96419861  1216765351297474560  ...              0                4\n",
              "\n",
              "[10378697 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLHc-ZS9CP0E",
        "outputId": "34296c4c-c34a-4ce6-aba8-d139fa85787a"
      },
      "source": [
        "list_tweet = []\n",
        "\n",
        "for site in data2:\n",
        "  user = TwitterUser(sname= site[2])\n",
        "  user.get_all_data()\n",
        "  for tweet in user.tweet:\n",
        "    list_tweet.append([tweet.user.id, tweet.id, tweet.text, tweet.retweet_count, tweet.favorite_count, site[1]])\n",
        "\n",
        "tableauTweets = pd.DataFrame(list_tweet, columns=['user_id', 'id_tweet', 'tweet', 'retweet_count', 'favorite_count', 'label_liability'])\n",
        "tableauTweets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for acrimed_info: 3247it [00:09, 329.80it/s]\n",
            "get_tweets for Charlie_Hebdo_: 3250it [00:06, 472.78it/s]\n",
            "get_tweets for EsanteFR: 3200it [00:06, 470.65it/s]\n",
            "get_tweets for PasseportSante: 3250it [00:07, 422.60it/s]\n",
            "get_tweets for Morandinisante: 3171it [00:07, 428.46it/s]\n",
            "get_tweets for lesucretvous: 3198it [00:09, 328.63it/s]\n",
            "get_tweets for electroallergik: 3141it [00:08, 358.61it/s]\n",
            "get_tweets for Allodocteurs: 3248it [00:07, 422.56it/s]\n",
            "get_tweets for AlterEco_: 3244it [00:09, 344.15it/s]\n",
            "get_tweets for mesopinions_com: 3246it [00:07, 442.68it/s]\n",
            "get_tweets for Change: 3250it [00:07, 440.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for Canardpcredac: 3249it [00:07, 408.93it/s]\n",
            "get_tweets for Confidentielles: 3200it [00:06, 498.23it/s]\n",
            "get_tweets for dirtybiology: 3224it [00:07, 419.33it/s]\n",
            "get_tweets for epenser: 3250it [00:07, 445.70it/s]\n",
            "get_tweets for Clubic: 3250it [00:06, 465.77it/s]\n",
            "get_tweets for journalDfemmes: 3234it [00:11, 289.67it/s]\n",
            "get_tweets for commentcamarche: 3250it [00:07, 442.21it/s]\n",
            "get_tweets for JVCom: 3250it [00:07, 425.50it/s]\n",
            "get_tweets for astucenaturelle:  31%|‚ñà‚ñà‚ñà       | 310/1000 [00:00<00:01, 366.90it/s]\n",
            "get_tweets for Santeplusmag: 3200it [00:06, 468.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for humourdedroite: 3235it [00:10, 306.47it/s]\n",
            "get_tweets for Maitre_Eolas: 3249it [00:07, 434.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception get_info\n",
            "Exception get_tweets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get_tweets for VosgesMatin: 3250it [00:12, 253.72it/s]\n",
            "get_tweets for VICEfr: 3249it [00:06, 471.61it/s]\n",
            "get_tweets for VICE: 3250it [00:09, 352.62it/s]\n",
            "get_tweets for USATODAY: 3250it [00:07, 415.95it/s]\n",
            "get_tweets for TVMAG: 3250it [00:07, 412.99it/s]\n",
            "get_tweets for washingtonpost: 3250it [00:07, 443.46it/s]\n",
            "get_tweets for WSJ: 3250it [00:06, 474.01it/s]\n",
            "get_tweets for nytimes: 3250it [00:08, 380.35it/s]\n",
            "get_tweets for theintercept: 3250it [00:06, 468.29it/s]\n",
            "get_tweets for Independent: 3250it [00:07, 428.04it/s]\n",
            "get_tweets for heraldscotland: 3250it [00:07, 407.92it/s]\n",
            "get_tweets for guardian: 3250it [00:07, 448.51it/s]\n",
            "get_tweets for TheEconomist: 3250it [00:07, 459.84it/s]\n",
            "get_tweets for dailytelegraph: 3250it [00:06, 468.87it/s]\n",
            "get_tweets for TF1: 3246it [00:08, 396.49it/s]\n",
            "get_tweets for TETUmag: 3246it [00:07, 438.35it/s]\n",
            "get_tweets for Telerama: 3250it [00:08, 371.43it/s]\n",
            "get_tweets for TeleLoisirs: 3250it [00:07, 463.03it/s]\n",
            "get_tweets for Tele7: 3250it [00:08, 370.09it/s]\n",
            "get_tweets for sudouest: 3250it [00:07, 420.78it/s]\n",
            "get_tweets for streetpress: 3248it [00:08, 400.71it/s]\n",
            "get_tweets for snopes: 3250it [00:08, 396.22it/s]\n",
            "get_tweets for Slatefr: 3250it [00:08, 368.08it/s]\n",
            "get_tweets for Sciences_Avenir: 3250it [00:07, 426.36it/s]\n",
            "get_tweets for science_et_vie: 3250it [00:07, 437.21it/s]\n",
            "get_tweets for Rue89: 3246it [00:07, 407.56it/s]\n",
            "get_tweets for RTLFrance: 3250it [00:07, 433.77it/s]\n",
            "get_tweets for RFI: 3250it [00:07, 406.90it/s]\n",
            "get_tweets for Reuters: 3250it [00:08, 372.03it/s]\n",
            "get_tweets for Reporterre: 3250it [02:06, 25.73it/s]\n",
            "get_tweets for Psychologies_: 3249it [00:07, 407.29it/s]\n",
            "get_tweets for presseocean: 3250it [00:07, 426.06it/s]\n",
            "get_tweets for PremiereFR: 3250it [00:09, 343.36it/s]\n",
            "get_tweets for Politis_fr: 3245it [00:09, 334.24it/s]\n",
            "get_tweets for POLITICOEurope: 3250it [00:06, 473.31it/s]\n",
            "get_tweets for politico: 3250it [00:07, 425.24it/s]\n",
            "get_tweets for pelerincom:   1%|          | 12/1000 [00:00<00:30, 32.40it/s]\n",
            "get_tweets for paris_normandie: 3250it [00:07, 413.13it/s]\n",
            "get_tweets for ParisMatch: 3250it [00:07, 435.33it/s]\n",
            "get_tweets for OuestFrance: 3250it [00:06, 474.71it/s]\n",
            "get_tweets for OnzeMondial: 3250it [00:07, 460.26it/s]\n",
            "get_tweets for Numerama: 3250it [00:07, 436.48it/s]\n",
            "get_tweets for NotreTemps: 3247it [00:07, 426.72it/s]\n",
            "get_tweets for NordEclairWeb: 3250it [00:07, 425.02it/s]\n",
            "get_tweets for NonSpolitique: 2049it [00:05, 346.07it/s]\n",
            "get_tweets for Nice_Matin: 3250it [00:07, 424.45it/s]\n",
            "get_tweets for NYDailyNews: 3250it [00:07, 425.12it/s]\n",
            "get_tweets for mouv: 3250it [00:20, 158.62it/s]\n",
            "get_tweets for VotreArgent: 1783it [00:04, 428.63it/s]\n",
            "get_tweets for Midilibre: 3250it [00:07, 448.86it/s]\n",
            "get_tweets for Mediapart: 3248it [00:08, 382.86it/s]\n",
            "get_tweets for MashableFR:   0%|          | 3/1000 [00:00<01:43,  9.64it/s]\n",
            "get_tweets for mashable: 3250it [00:08, 368.99it/s]\n",
            "get_tweets for mariefrancemag: 3250it [00:07, 459.52it/s]\n",
            "get_tweets for marieclaire: 3249it [00:07, 456.92it/s]\n",
            "get_tweets for MarianneleMag: 3250it [00:09, 360.49it/s]\n",
            "get_tweets for Madamefigaro: 3250it [00:07, 409.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhGQXIHq8wC"
      },
      "source": [
        "R√©initialisation du fichier PauchBas.pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YGrwhVLp-XM"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(DATA_PATH+\"PaucBas.pickle\", \"wb\") as obj_file:\n",
        "    pickle.dump([], obj_file, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhSJo6vwG8kn"
      },
      "source": [
        "Sauvegarde des donn√©es re√ßu pour chaque utilisateur Twitter recherch√©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J7FWlgrrFTx"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data1:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uqFx3lwrFEU"
      },
      "source": [
        "# with open(DATA_PATH+\"PaucBas.pickle\", \"ab\") as obj_file:\n",
        "#   for site in data2:\n",
        "#     user = TwitterUser(sname= site[2])\n",
        "#     user.get_all_data()\n",
        "#     pickle.dump([user], obj_file, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}